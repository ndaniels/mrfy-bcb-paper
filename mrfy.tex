\documentclass{acm_proc_article-sp}


%\usepackage{scicite}
\usepackage{amsmath}

\usepackage{graphicx}
\usepackage{psfrag}
\usepackage{subfigure}
\usepackage[usenames]{color}
\usepackage[table]{xcolor}    % loads also colortb
\usepackage{array}
\usepackage{url}
\usepackage{amssymb}
\usepackage{natbib}
% \usepackage{amsthm}


\usepackage{times}

\renewcommand\ttdefault{aett}

  \long\def\remark#1{%
      \ifvmode
         \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \else
          \unskip\raisebox{-3.5pt}{\rlap{$\scriptstyle\diamond$}}%
          \marginpar{\raggedright\hbadness=10000
         \parindent=8pt \parskip=2pt
         \def\baselinestretch{0.8}\tiny
         \itshape\noindent #1\par}%
      \fi}


%\topmargin 0.0cm
%\oddsidemargin 0.2cm
%\textwidth 16cm 
%\textheight 21cm
%\footskip 1.0cm

\begin{document}

\title{MRFy: Remote Homology Detection for Beta-Structural Proteins
  Using Markov Random Fields and Stochastic Search}

\author{Noah M. Daniels \hspace*{8pt} Andrew Gallant \hspace*{8pt} Norman 
Ramsey \hspace*{8pt} 
Lenore Cowen\thanks{Corresponding author: lenore.cowen@tufts.edu}
\\ Department of Computer Science, Tufts University \\ 161 College Ave, 
Medford, MA 02155.}

\date{\today}

\maketitle



%%%%%%%%%%%%%%%%% END OF PREAMBLE %%%%%%%%%%%%%%%%

\begin{abstract}

We introduce MRFy, a tool for protein remote homology detection that captures
beta-strand dependencies in the Markov random field.
On a diverse set of SCOP beta-structural superfamilies, MRFy shows a mean 20\%
improvement in Area Under the Curve as compared to HMMER, 34\% improvement as
compared to RAPTOR, 17\% improvement as compared to HHPred, and a 32\% 
improvement as compared to CNFPred.
MRFy was implemented in the Haskell functional programming language, and
parallelizes well on multi-core systems.
MRFy is available, as source code as well as an executable, from
\url{http://mrfy.cs.tufts.edu/}.

\end{abstract} 

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%
% BEGIN MAIN TEXT
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%



\section{Introduction}

Computational approaches to the problem of recognizing remote homologs of
well-annotated protein structures from sequence have emerged as some of the most
successful high-throughput strategies for developing hypotheses about the
functions of unknown proteins.
Recognition of remote homologs from protein sequence is a challenging problem,
however, particularly for $\beta$-structural motifs, where many of the
residues whose interactions drive the fold can be a variable, and sometimes
long, distance apart in sequence~\cite{Menke:2010ti}. 
Many of the popular
sequence-based methods for recognizing close homologs, such as Profile
Hidden Markov Models (HMMs)~\cite{Eddy:1998ut,Hughey:1996ub}, perform 
particularly poorly on
$\beta$-structural motifs, precisely because the HMM model is not
powerful enough to capture these long-range 
dependencies~\cite{Lifson:1980vd, Zhu:1999wr, Olmea:1999ug, 
Cowen:2002p588,Steward:2002wz}. 
Several ways
to generalize these HMMs to Markov random fields (MRFs) have therefore
been 
proposed~\cite{White:1994ty, Lathrop:1996gm, Thomas:2008uw, 
Gopalakrishnan:2009wx, Menke:2010ti, Peng:2011vk}, but with the additional 
power of the 
random field comes several computational challenges. 
In particular, there is a trade-off: as the complexity of the random field
increases, so does its predictive power, but so too do the required amount of 
training data and the computational complexity of aligning a query sequence
to the model.

In 2010, Menke, Berger and Cowen introduced
SMURF)~\cite{Menke:2010ti}, a Markov Random Field method designed to
recognize protein sequences that would fold into $\beta$-propeller
shapes. 
While SMURF greatly improved $\beta$-propeller recognition over
existing HMM and threading methods, it was actually quite simple as
far as MRFs are concerned: it combined a score based on a standard profile 
hidden Markov model with a conditional probability score that incorporated
the statistical preferences of the amino acid residues that are
hydrogen-bonded in $\beta$-sheets, thereby capturing some of the
strongest long-range dependencies in $\beta$-structural motifs.
The
pairwise statistical preferences were learned from solved
$\beta$-structures across the entire PDB, so there was sufficient
training data. 
However, there was a great computational cost: the
minimum-energy alignment of the sequence to the Markov random field model
was generated exactly using a doubly-exponential multi-dimensional
dynamic program.


This computational cost was the barrier that prevented the SMURF MRF
from being applied beyond propellers to other $\beta$-structures.
When run on $\beta$-barrels, or other $\beta$-structures with
even moderately complex strand interleaving patterns, the exact
computation of the SMURF MRF becomes intractable. 
There are two ways to mitigate the computational bottleneck. 
One approach is to simplify the random field, and only consider some of the 
pairwise statistical preferences for the $\beta$-strands, namely those that are more local in 
sequence.
This is the methodology we introduced in a 2012 paper of Daniels, Hosur, Berger
and Cowen, when we designed SMURFLite~\cite{Daniels:2012dg}. 
The other approach is to retain the entire SMURF MRF, but consider
stochastic search approaches to heuristically explore alignments of the
sequence to the MRF, in order to find low-energy but not necessarily
minimum-energy parses.
This is the approach we take in the present work.

In particular, we introduce MRFy, a suite of methods to perform
stochastic searches of the SMURF MRF. 
We show that MRFy, combined with Kumar and Cowen's 
{\em simulated evolution\/}~\cite{Kumar:2009tp, Kumar:2010wv}
outperforms HMMer (a popular HMM method), Raptor (a popular threading method), 
CNFPred (a state-of the art threading 
method), and HHPred (a profile-profile HMM method that has performed well at 
recent CASP competitions) at recognizing different $\beta$-barrel superfamilies
in a stringent leave-family-out cross validation experiment. 
MRFy is available for download at \url{http://mrfy.cs.tufts.edu}. 
It is written in the Haskell functional programming language; some details of 
implementation are further discussed in~\cite{Daniels:2012cm}.

Finally, we find the best approach is a hybrid combination of
SMURFLite and MRFy: we initialize the MRFy stochastic search from the
best sequence parse that SMURFLite can find, and then run the
stochastic search from there. On the same set of $\beta$-barrel superfamilies 
considered in \cite{Daniels:2012dg}, we show a mean 3.5\% (median 2.2\%) 
improvement in Area Under Curve (AUC) for
$\beta$-structural motif recognition as compared to the SMURFLite results 
in~\cite{Daniels:2012dg}.
By these same benchmarks, we show a mean 20\% (median 10\%) improvement over
HMMER (\cite{Eddy:1998ut}), a mean 34\% (median
23\%) improvement as compared to RAPTOR (\cite{Xu:2003p3417}),
a mean 32\% (median 19\%) improvement over CNFPred~\cite{JianzhuMa:2012cp},
and a mean 17\% (median 21\%) improvement in AUC 
over HHPred (\cite{Soding:2005ff}).



\section{Methods}

\subsection{Markov random field model}

MRFy builds on the SMURF and SMURFLite Markov random field 
model~\cite{Daniels:2012dg}, which uses multidimensional 
dynamic programming to simultaneously capture both standard HMM models and the
pairwise interactions between amino acid residues bonded together in
$\beta$-sheets. 

In particular, the ``Plan7'' hidden Markov model is modified to represent
hydrogen-bonded $\beta$-strands with additional, non-local edges.
Because the $\beta$-strands in a SMURF or MRFy template represent 
\emph{consensus} 
$\beta$-strands, those present in at least some fraction (in our experiments, 
at least
half) of the sequences participating in the training alignment, we prohibit
insertions and deletions in those strands.
Thus, we collapse those nodes of the ``Plan7'' model to be just match states;
the transitions to insertion and deletion states are removed.
Figure~\ref{mrfy_model} illustrates this architecture.


\begin{figure}[htb!]
\begin{center}
  \fbox{\includegraphics[width=2.8in]{mrf_interleave_diagram.pdf}}
   \caption{A MRFy Markov random field with two $\beta$-strand pairs}
   \label{mrfy_model}
 \end{center}
\end{figure}

The standard form of the Viterbi
recurrence relations for computing the most likely path of a sequence through
a hidden Markov model, as incorporated in HMMER~\cite{Eddy:1998ut} is:
\newcommand\txprobj[3][]{a#1_{{#2}_{j-1}{#3}_j}}
\newcommand\txprobjj[3][]{a#1_{{#2}_{j-1}{#3}_j}}
\newcommand\alignwidth{\ensuremath C} % number of columns in an alignment
\newcommand\pairedwith[1]{{\pi(#1)}}
\newcommand\vsum[2]{#2&{}+{}& #1}
\def\goo{18pt}
\def\gum{14pt}
\def\maxiquad{\hskip 1.2em\relax}
\begin{equation*}
\begin{array}{@{}l@{}c@{}l}
V_{j}^{M}(i) &{}={}& \frac{e_{M_{j}}(x_{i})}{q_{x_{i}}} \times \max \left\{
  \begin{array}{l@{}c@{}l}
  V_{j-1}^{M}(i - 1) \times a_{M_{j-1}M_{j}}\\
  V_{j-1}^{I}(i - 1) \times a_{I_{j-1}M_{j}}\\
  V_{j-1}^{D}(i - 1) \times a_{D_{j-1}M_{j}}\\
  \end{array} \right.\\[\goo]
V_{j}^{I}(i) &=& \log\frac{e_{I_{j}}(x_{i})}{q_{x_{i}}} \times \max \left\{
  \begin{array}{l@{}c@{}l}
  V_{j}^{M}(i - 1) \times a_{M_{j}I_{j}}\\
  V_{j}^{I}(i - 1) \times a_{I_{j}I_{j}}\\
  \end{array} \right.\\[\gum]
V_{j}^{D}(i) &=& \max \left\{
  \begin{array}{l@{}c@{}l}
  V_{j-1}^{M}(i) \times a_{M_{j-1}D_{j}}\\
  V_{j-1}^{D}(i) \times a_{D_{j-1}D_{j}}\\
  \end{array} \right.\\
\end{array}
\end{equation*}

In the SMURF or MRFy Markov random field model, we add non-local interactions 
to these
probabilities, resulting in conditional probabilities.
When column~$j$ of an alignment is part of a $\beta$-strand and is paired
with another column  $\pairedwith j$,
the probability of finding amino acid~$x_i$ in column~$j$ 
depends on whatever amino acid~$x'$  is in column~${\pairedwith i}$.
If~$x'$ is in position~$i'$ in the query sequence, Viterbi's
equations are altered; for example,
$V_{j}^{\prime M}(i)$ depends not only on
$V_{j-1}^{\prime M}(i-1)$ but also on
$V_{\pairedwith j}^{\prime M}(i')$.
The distance between $j$~and~$\pairedwith j$ can be as small as a few
columns or as large as a few hundreds of columns.
Because $V_j^{\prime M}(i)$~depends not only on nearby values but also on
$V_{\pairedwith j}^{\prime M}(i')$,
we must modify the Viterbi recurrence relations.

Note that hydrogen-bonded $\beta$-strand residues may only occupy match states 
in the Markov random field, so only the corresponding terms of the recurrence
relation need be modified.
The revised Viterbi recurrence relation for the Markov random field is:
\begin{small}
\begin{multline}
V_{j}^{M}(i) = \frac{e_{M_{j}}(x_{i})}{q_{x_{i}}} \times \notag
\\\notag\\ \max \left\{
  \begin{array}{l@{}c@{}l}
  V_{j-1}^{M}(i - 1) \times a_{M_{j-1}M_{j}} \times P(x_{i}|x_{\pi j})\\
  V_{j-1}^{I}(i - 1) \times a_{I_{j-1}M_{j}} \times P(x_{i}|x_{\pi j})\\
  V_{j-1}^{D}(i - 1) \times a_{D_{j-1}M_{j}} \times P(x_{i}|x_{\pi j})\\
  \end{array} \right.\\
\end{multline}
\end{small}
where $x_{\pi j}$ represents the amino acid in column $\pi j$, which is 
hydrogen-bonded to the amino acid $x_{i}$ in column $j$. Then, transforming
to negative logspace, we actually solve: 
%For reasons of convenience, as well as avoiding floating-point underflow due to
%exceedingly small numbers, we typically work in negative log space.
%Since a probability can range from 0 to 1, the log of a probability must be a
%negative number, and thus the negative log of that probability is a (small)
%positive number.
%Each probability is transformed into its negative log, resulting in the final
%form:
\begin{small}
\begin{multline}\label{viterbi-final}
V_{j}^{\prime M}(i) = e^{\prime}_{M_{j}}(x_{i}) + \notag
\\\notag\\ \min \left\{
  \begin{array}{l@{}c@{}l}
  \vsum{V_{j-1}^{\prime M}(i-1)} {a^{\prime}_{M_{j-1}M_{j}}} + P'(x_{i}|x_{\pi j})\\
  \vsum{V_{j-1}^{\prime I}(i-1)} {a^{\prime}_{I_{j-1}M_{j}}} + P'(x_{i}|x_{\pi j})\\
  \vsum{V_{j-1}^{\prime D}(i-1)} {a^{\prime}_{D_{j-1}M_{j}}} + P'(x_{i}|x_{\pi j})\\
  \end{array} \right.\\\\
V_{j}^{\prime I}(i) = e^{\prime}_{I_{j}}(x_{i}) + \min \left\{
  \begin{array}{l@{}c@{}l}
  \vsum{V_{j}^{\prime M}(i - 1)} {a^{\prime}_{M_{j}I_{j}}}\\
  \vsum{V_{j}^{\prime I}(i - 1)} {a^{\prime}_{I_{j}I_{j}}}\\
  \end{array} \right.\\\\
V_{j}^{\prime D}(i) = \min \left\{
  \begin{array}{l@{}c@{}l}
  \vsum{V_{j-1}^{\prime M}(i)} {a^{\prime}_{M_{j-1}D_{j}}}\\
  \vsum{V_{j-1}^{\prime D}(i)} {a^{\prime}_{D_{j-1}D_{j}}}\\
  \end{array} \right.\\
\end{multline}
\end{small}
given the transformations:
\begin{equation*}
  \begin{array}{l@{}c@{}l@{}}
  a'_{s \hat{s}} = - \log a_{s \hat{s}} \\
  e^{\prime}_{s}(x) = - \log\frac{e_{s}(x)}{q_{x}}\\
  V_j^{\prime M}(i) = - \log V_j^{M}(i)\\
  P^{\prime}(x_{i}|x_{\pi j}) = - \log P(x_{i}|x_{\pi j})
  \end{array}\\
\end{equation*}
This is exactly the recurrence relation that SMURF~\cite{Menke:2010ti} and 
SMURFLite~\cite{Daniels:2012dg} solve using multidimensional dynamic 
programming.
As demonstrated by Daniels, et al.~\cite{Daniels:2012dg}, as the 
\emph{interleave} 
of the $\beta$-strands increases, the computational complexity grows 
exponentially.

As an alternative to exactly solving these more complex recurrence
relations, we might consider a divide-and-conquer approach.  Each
$\beta$-strand can be thought of as breaking the larger model into two
smaller models; collectively, all the $\beta$-strands divide the
Markov random field into many small, \emph{independent} hidden Markov
models.  Thus, for any particular path through the Markov random
field, corresponding to a particular placement of query sequence
residues onto the nodes of the model, we could compute the augmented
Viterbi score by summing the Viterbi scores of each smaller hidden
Markov model, along with the contribution to the Viterbi score from
the $\beta$-strands.

Since only match states are allowed for $\beta$-strand residues, the 
contribution of each such residue is only:
\begin{multline*}
  V_{j}^{\prime M}(i) = e^{\prime}_{M_{j}}(x_{i}) + \notag\\
V_{j-1}^{\prime M}(i - 1) + a^{\prime}_{M_{j-1}M_{j}} + P(x_{i}|x_{\pi j})
\end{multline*}
The asymptotic complexity of the Viterbi algorithm is $O(mn)$, where $m$ is the
length of the model and $n$ is the length of the query sequence.
Furthermore, the asymptotic complexity of the beta-strand contribution to the
Viterbi score for a particular placement of residues is just $O(b)$, where $b$
is the combined length of the $\beta$-strands.

Thus, a new algorithm for computing the optimal path through a Markov random 
field for a given query sequence presents itself.
Since we require that every $\beta$-strand position be occupied by a residue 
(as we force those positions into match states), we could simply consider every
possible assignment of a residue to a $\beta$-strand, computing the score for 
each one, and choose the best-scoring placement.

Metaphorically, we can picture the residues of the query sequence as beads,
and the Markov random field as the string of a necklace.
The $\beta$-strands can be thought of as particular substrings of the string 
that
must be covered by beads, while non-$\beta$ regions may be exposed (resulting in
delete states in the model).
To continue the metaphor, we may jam extra beads onto non-$\beta$ regions of the
string, resulting in insert states in the model.
Given that the beads already have a specified order, we must consider all the
ways to slide the beads up and down the string such that all of the 
$\beta$-regions
are covered.
Since the regions between $\beta$-strands can have their contribution to the
score computed according to the Viterbi recurrence relations, we need only
consider all the unique ways to assign residues to the $\beta$-strand nodes.

However, it is easy to show that there are an exponential number of such
assignments.

Let a Markov random field model $(N,B)$ be defined as a sequence $N$ of nodes 
$n_{i}, i \in (1..m)$, and a sequence $B$ of $\beta$-strands 
$b_{i}, i \in (1..k)$.
Each $\beta$-strand has length $l_{i}$, and contains a subsequence of the nodes
$N$.
This subsequence is determined by the
specifics of the model, which can be referred to as $b_{ij}, i \in (1..m), j \in
(1..l_{i})$.
Let a query sequence be defined as a sequence $R$ of residues
$r_{i}, i \in (1..n)$.
Let $L = \displaystyle \sum \limits_{i, i <= k} l_{i}$.

There are:
\begin{equation*}
  \prod_{i \in (1..k)}{n - L - (i \times l_{max})} = 
  (n - 2L - k\times(l_{max}))^{\lceil \frac{k}{2} \rceil}
\end{equation*}
possible placements of $R$ onto $(N,B)$.
Asymptotically, as $n$ grows, this is dominated by $n^k$, leading to an
asymptotic complexity of $O(n^k)$.

A typical Markov random field might have 10 or 20 $\beta$-strands, and a typical
protein query sequence might have between 300 and 600 residues.
Thus, if we wish to consider all possible paths through a Markov random field
for a protein sequence, we must consider as many as 
$600^{10} \approx 6 \times 10^{27}$ possible paths through the model.

\subsection{Stochastic search}

Since an exhaustive search for an optimal alignment of a protein sequence to a
Markov random field is exponential in complexity, we turn to stochastic search
to mitigate this complexity.

Framed as an optimization problem, MRFy, like SMURF, seeks to minimize the 
augmented Viterbi
score, which equates to maximizing 
probability (recall that this score is the negative log of a probability).
SMURF~\cite{Menke:2010ti} finds this minimum exactly, using multi-dimensional dynamic programming,
which is exponential in the interleave number of beta strands.
MRFy, in contrast, uses stochastic search, as described next.

Given a placement of query-sequence residues into $\beta$-strand nodes of the
Markov random field, the score can be computed exactly.
Thus, the search space is the set of all possible ways to place residues on
these nodes.
MRFy, as a package, implements many different stochastic search strategies for
finding the optimal alignment of a query sequence to the Markov random field.
A particular MRFy strategy involves specifying three different criteria:
first, how the initial guess is generated (four options), second, how the
stochastic search proceeds (we include a simulated annealing, genetic algorithm,
and local search option), and finally, when to terminate and output the search
and output the best alignment seen thus far (three options). We first describe the intial guess options, then the termination options, and finally the options for stochastic search implemented in MRFy. 

%Many stochastic search techniques rely on a gradient ascent (or descent)
%approach, which makes moves (or refines guesses) along the steepest gradient,
%leading quickly to local optima; various heuristics such as simulated 
%annealing~\cite{Kirkpatrick:1983wa} can then help avoid getting stuck in poor
%local optima.

%However, we know of no way to compute a gradient on the search space of
%$\beta$-strand placements, and so we must take approaches that do not rely
%on this gradient.
%Instead, we must rely on a random-mutation model of search, which generates one
%or more candidate solutions (guesses) from a previous solution, and then
%nevaluates the cost function (in our case, the augmented Viterbi score) to
%determine whether those guesses are better or worse than the previous step.
%This can be likened to climbing a hill in the dark, feeling one's way with one 
%foot before committing to a step.
%This approach is referred to as \emph{random-mutation hill
% climbing}\cite{Davis:1991vx}.


%In our representation, a particular solution is represented by an ordered list
%of integers, one integer per $\beta$-strand in the Markov random field.
%The value of each integer indicates the index, in the query sequence, of the
%residue assigned to the first position of that $\beta$-strand.
%Since the alignments to the regions of the Markov random field are solved
%exactly by the Viterbi algorithm, this ordered list of integers uniquely
%represents a solution to a Markov random field.

%While the picture we have presented for our Markov random field model is most
%precisely explained by assigning residue indices to the positions of 
%$\beta$-strands, it may be more intuitive to consider the equivalent problem
%of ``sliding'' these $\beta$-strands along the query sequence.
%We will use this analogy in the following description of initial guesses.

We explored four models for generating initial guesses for our search
techniques.
A \emph{random-placement} model uniformly positions 
the $\beta$-strands
along the query sequence, under the constraint that only legal placements
may be generated, and thus the placement of any $\beta$-strand must leave 
room for all the other $\beta$-strands in the model.

A \emph{secondary-structure} prediction model uses the 
PSIPRED~\cite{McGuffin:2000wx} 
secondary-structure
prediction program to determine the positions of $\beta$-strands.
Given a PSIPRED prediction for the secondary structure of a query sequence,
we place $\beta$-strands at the most likely locations according to this
prediction profile, randomized by a small amount of noise.
% The difficulty with this approach is that, while PSIPRED is reasonably 
% accurate when it is allowed to perform PSI-BLAST\cite{Altschul:1997tl} 
% queries to build a sequence
% profile, this comes at a run-time cost that completely dominates the running
% time of MRFy.
% However, while non-profile-based PSIPRED predictions are computationally 
% cheap, they provide poor accuracy.
  
A \emph{template-based model} is based on the observation 
that true homologs
to a structurally-derived template should have their $\beta$-strands in very
roughly similar places, in sequence, to the proteins that made up that
template.
This will not always hold, but appears to provide for reasonable initial
guesses.
Given the position of each $\beta$-strand within a template Markov random 
field, we scale the query sequence linearly (as it may be shorter or longer 
than the model) and place the $\beta$-strands in scaled positions.
Note that we do not scale the $\beta$-strands themselves; their lengths are
preserved.
We scale only the distances between $\beta$-strands.
We inject a small amount of noise into the placements, so that 
population-based models, such as multi-start simulated annealing and genetic
algorithms, start with heterogenous solutions.
  
Finally, we implemented a model that attempts to unify the SMURFLite and
MRFy approaches.
Given a template $T$, we create a new template, $T^\prime$, exactly as in
\citet{Daniels:2012dg}, from which we have removed any $\beta$-strand pairs
that exceed an interleave threshold of 2.
We then run SMURFLite on $T^\prime$, to create an alignment $a^\prime$.
Note that $a^\prime$ is an optimal solution to the template $T^\prime$.
We wish to use $a^\prime$ as an initial guess for MRFy, but we must first add
in the $\beta$-strands that were removed.
In order to do so, we greedily move the placement of each $\beta$-strand in
$a^\prime$ as needed to make room for those other $\beta$-strands.
The resulting alignment $a$ becomes the initial guess for MRFy.


Since we do not know how to determine when a stochastic search process has found
a \emph{global} optimum (as opposed to a good local optimum), we must also have
some termination criterion for the search.
We implemented three alternative termination criteria. The first is a simple 
generation-counting approach, where the search terminates after
a user-specified number of generations.
The second is a time-based approach, where the search terminates after a 
user-specified amount of time has elapsed.
Finally, a \emph{convergence} model terminates after the search
has failed to improve after a user-specified number of generations.
  
In practice, these criteria are easily combined, with a convergence approach
often halting searches early with good results, while the generation- or 
time-based limit ensuring that the search does not take longer than a user is
willing to wait.
We next describe the alternative heuristics that MRFy implements for stochastic
search: simulated annealing, a genetic algorithm, and a local search strategy.


\subsubsection{Simulated Annealing}

Simulated annealing~\cite{Kirkpatrick:1983wa} is a heuristic for stochastic search,
inspired by the physical process of annealing in metals.
Whereas a simple hill-climbing approach will always terminate at the first 
local optimum encountered,
simulated annealing introduces an \emph{acceptance probability function}:
\begin{equation*}
  \begin{split}
  P(e,e',T) = \begin{cases}
    1, & \text{ if } e' < e\\
    \exp(-(e'-e)/T),& \text{ otherwise }\\
\end{cases}\\
  \text{ where }
     e = E(s) \\
     e' = E(s')\\
  \end{split}
\end{equation*}
which relies on some energy function $E(s)$ of the current state $s$ and
a candidate state $s'$, and a temperature function $T$ that tends towards zero
as the search progresses.
In our implementation, we used an exponentially-decaying temperature function:
\begin{equation*}
  T(t) = k^{t}\times T_{0}
\end{equation*}
given time $t$, initial temperature $T_{0}$, and a constant $k$.
The motivation for this decaying temperature function is that, as time 
progresses, the likelihood of being in a \emph{poor} local optimum lessens, and
thus, the closer to random hill-climbing we would like the search to behave.


Our energy function $E(s)$ is, naturally, the augmented Viterbi score of a
placement:
\begin{equation*}
  E(s) = V_{m}^{\prime M}(n)
\end{equation*}
where $m$ is the final residue in the query sequence and $n$ is the final
node in the Markov random field, and the $\beta$-strand placements are 
determined by $s$.

We implemented simulated annealing in MRFy according to this model.
We also implemented a \emph{multi-start} version of simulated annealing in MRFy,
where a set of independently-generated guesses is subject to simulated-annealing
random descent, in parallel.
At the termination of the search, the best solution from among all the 
candidates is chosen.


\subsubsection{Genetic Algorithm}

A genetic algorithm~\cite{Holland:1977hl} relies on the idea of 
\emph{selection} among a population of
varied solutions to an optimization problem.
At each of many generations, the fitter individuals in the population--those
solutions which exhibit more optimal scores--are allowed to continue into the
next generation.
Not only do they continue into the next generation, but they are allowed to
``reproduce,'' or recombine, to produce new solutions.
A particular solution to a problem, within the context of a genetic algorithm,
is called a \emph{chromosome}.
At each generation, some fraction of the fittest solutions are selected and
randomly paired with one another.
Each pair of solutions produces one or more offspring; each offspring is the
result of two steps: \emph{crossover} of the two chromosomes, followed by
random \emph{mutation} of the offspring.
The mutation is nondeterministic; the crossover may be deterministic or
nondeterministic.
The resulting offspring, along with their parents, are then evaluated according
to the objective function, and this process iterates until some termination
condition.


MRFy's genetic algorithm implementation uses the same representation for a
placement as simulated annealing: an ordered list of integers.

Let a \emph{placement} $p$ on a model with $k$ $\beta$-strands be an ordered 
set of integers $p_{i}, i \in (1..k)$.
Given two placements, $p$ and $q$, MRFy implements crossover of two chromosomes 
using the following algorithm:

\begin{enumerate}
  \item Set the new placement, $p'$, to the empty set.
  \item Repeat until all placements have been chosen:
  \begin{enumerate}
    \item Set the position for $p'_{0}$ to $p_{0}$
    \item Set the position for $p'_{k}$ to $q_{k}$
    \item Remove $p_{0}$, $p_{k}$, $q_{0}$, and $q_{k}$
    \item Adjust indices so $p$ and $q$ now range from what had been 1 to $k-1$.
  \end{enumerate}
\end{enumerate}

Our actual implementation is purely functional, and simply consumes elements 
from lists.
In effect, though, this algorithm simply chooses the `left-most' elements from 
one parent and the `right-most' elements from another.
After crossover, the mutation step simply moves each element $p_{i}$ of the
placement $p$ by a small, random amount, within the constraints imposed by the
neighboring $\beta$-strands.
The motivation behind this approach is to take two solutions that are of high
fitness (recall that the worst solutions at every generation are not allowed
to contribute to the next generation), and produce a new solution that combines
one ``half'' (roughly) of one solution with one ``half'' of the other.
See Figure~\ref{crossover} for an illustration of this procedure.

\begin{figure}[htb!]
\begin{center}
  \fbox{\includegraphics[width=2.8in]{crossover.pdf}}
   \caption{The crossover and mutation process in MRFy's genetic algorithm 
   implementation.
   Given parent $p$ (black) and parent $q$ (gray), alternate left and right
    placements from $p$ and $q$.
    Then, apply small random mutations to the resulting placement $p'$.}
   \label{crossover}
 \end{center}
\end{figure}


Given these operations for crossover and mutation, MRFy's genetic algorithm
implementation initializes a population of a user-specified size $P$ (typically 
one thousand placements, though we experimented with as many as ten thousand).
In parallel, each placement is scored according to the objective function.
Since scoring is far more computationally expensive than crossover and mutation,
we allow them all to reproduce, paired at random.
We then score them, and choose the $P$ best-scoring placements for the next
generation.
This process repeats until a termination condition is met, at which point the
single best placement is returned.
We note that a future enhancement to MRFy could return the $k$ best placements
for some user-specified threshold $k$, if multiple high-scoring alignments were
to be considered.


\subsubsection{Local Search}\label{localsearch}

Constraint-based local search~\cite{Hentenryck:2009vn} is a family of approaches
for exploring ``neighborhoods'' in feature space in a randomized manner, 
subject to the constraints of that solution space.
In the context of MRFy, the constraints are the previously-discussed 
restrictions that $\beta$-strands cannot overlap, and every residue must be
placed in a $\beta$-strand.
Given a single candidate solution, local search explores the immediate 
neighborhood in great detail (perhaps, but not necessarily exhaustively).
When the local search cannot escape a local optimum, then some sort of 
\emph{non-local} move may be attempted.

This non-local move may rely on a population-based diversification approach,
in which parts of the solution may change dramatically.
In a sense, local search bears some resemblance to a genetic algorithm,
except that a population of solutions is created only when the search is stuck
in a local optima, and the best solution in that population is chosen for a new
search.

In MRFy's implementation, each step in the search consists of two phases: 
\emph{diversification} (See Figure~\ref{diversification}) and 
\emph{intensification}.
The diversification algorithm is as follows:
Begin with a candidate solution $s$ (a placement), which is just
an ordered list of integers.
Given $s$, break the list into three sub-lists $s_{0}, s_{1}$, $s_{2}$, 
at randomly-chosen boundaries.
Choose one of the sub-lists $s_{i}$ at random, and mutate it into $k$ 
copies $s_{i1}$ through $s_{ik}$ at random, for some user-defined value of $k$ 
(we used $k=10$), within
the constraints imposed by the other sub-lists and the lengths of the 
$\beta$-strands.
Re-combine each set of lists, $(s_{1j}, s_{2j}, s_{3j})$ into a new 
placement $s^{\prime}_{j}, j \in (1..k)$.
Score each placement $s^{\prime}_{j}$, return the best-scoring of the $k$ new 
placements as a new solution.

Once diversification produces a new candidate solution, intensification brings
it toward a local maximum.
The intensification algorithm is as follows:
Begin with a candidate solution $s$.
Until no better-scoring placements are generated, repeat the following steps:
For each element $e \in s$, generate four new placements $s'_{i1}$ 
through $s'_{i4}$ by moving $e$ up and down by 1 and two, as long as those 
moves do not violate the constraints.
Next, score each candidate placement $s'_{ij}$.
Finally, set $s$ to the best-scoring candidate placement $s'_{ij}$.
Upon termination, return $s$ as a new solution.

\begin{figure}[htb!]
\begin{center}
  \fbox{\includegraphics[width=2.8in]{localsearch.pdf}}
   \caption[The diversification step in local search.]
   {The diversification step in local search.
   In this example, $\beta$-strand placements in the middle of the query
   sequence, corresponding to residue positions 24, 31, and 47, are varied
   to explore a diverse areas of the search space from a locally optimum 
   solution.}
   \label{diversification}
 \end{center}
\end{figure}

% 
% These are from sandwich
% 
% [1,17,24,31,47,56,68,88,120,137,149,161,173,186,202,219,224] 554.4214648689649
% [1,17,24,31,47,56,68,88,120,128,136,140,150,160,202,219,224] 650
% 
% they can be divided as follows;
% [1,17,24,31,47,56,68,88,120] -> no change
% [128,136,140] -> 10 difference
% [150,160] -> 23 difference
% [202,219,224] -> No change

%%

\subsection{Evaluating search strategies}\label{searchstrats}

As MRFy supports three significantly different stochastic search strategies, and
a number of tunable parameters such as termination conditions and (for 
simulated annealing) the cooling schedule, we conducted a search over parameter
space using a small data set.
We built Markov random field templates from the fold ``8-bladed Beta-Propellers'', and the superfamilies ``Barwin-like endoglucanases'' (a 
$\beta$-barrel superfamily) and ``Concanavalin A-like lectins/glucanases'' (a 
$\beta$-sandwich superfamily).
We were interested in the speed of convergence for a true-positive test case,
so we tested each template with a protein sequence chosen from that fold or
superfamily: for the 8-bladed propeller, we chose ASTRAL chain d1lrwa\_
(Methanol dehydrogenase, heavy chain from  \emph{Paracoccus denitrificans}).
For the barwin-like endoglucanases, we chose ASTRAL chain d2pica1 
(Membrane-bound lytic murein transglycosylase A, MLTA from \emph{E. coli}).
For the lectins/glucanases, we chose ASTRAL chain d2sbaa\_ 
(Legume lectin from soy bean (\emph{Glycine max})).

We tested simulated annealing with a population size of 10, a maximum number of
generations of 10000, convergence periods of 200, 500, and 1000 generations, 
and a cooling factor of 0.99 (preliminary tests showed little impact from
varying the cooling factor among 0.9, 0.99, and 0.999).

We tested the genetic algorithm implementation with a population size of 1000 
and 10000, a maximum number of generations of 500, and convergence
periods of 10, 50, and 100.

Since the local search distinguishes between diversification and 
intensification, counting the number of generations is ambiguous; we used a time
limit of 10 seconds, 30 seconds and 5 minutes.
All tests were conducted on a 12-core AMD Opteron 2427 with 32GB RAM, devoting 
all 12 cores to MRFy.
For each test, we report statistics based on ten runs for each set of 
parameters.

\subsection{Simulated Evolution} \label{mrfy-simev}

In MRFy, we incorporated precisely the same ``simulated evolution'' 
implementation, as first proposed by Kumar and 
Cowen~\cite{Kumar:2009tp, Kumar:2010wv}, as we did for SMURFLite
in~\cite{Daniels:2012dg}.
We added pairwise mutations based on $\beta$-strand pairings.
We use the same mutation frequencies as in \citet{Daniels:2012dg}.
For each training sequence, we generate 150 new artificial training 
sequences. 
For each of these artificial sequences, we mutate at a 50\% mutation rate per 
length of the $\beta$-strands.

\subsection{Datasets} \label{mrfy-datasets}

From SCOP (\cite{Murzin:1995uh}) version 1.75, we chose the same
$\beta$-barrel superfamiles as \citet{Daniels:2012dg}.
These superfamilies were: ``Nucleic acid-binding proteins'' (50249), 
``Translation proteins'' (50447), 
``Barwin-like endoglucanases'' (50685), ``Cyclophilin-like'' (50891), ``Sm-like 
ribonucleoproteins'' (50182), ``PDZ domain-like'' (50156), ``Prokaryotic 
SH3-related domain'' (82057), ``Tudor/PWWP/MBT'' (63748), ``Electron Transport 
accessory proteins'' (50090), ``Translation proteins SH3-like domain'' (50104), 
and ``FMN-binding split barrel'' (50475). 

\subsection{Training and testing process} \label{mrfy-training}

For the $\beta$-barrel superfamilies, we performed strict leave-family-out 
cross-validation. 
We built training templates at the superfamily level. 
For each superfamily, its constituent families were identified. 
Each family was left out, and a training set was established from the protein 
chains in the remaining families, with duplicate sequences removed. 
We built an MRF on the training set, both with and without training-data
augmentation using the same ``simulated evolution'' implementation as 
\citet{Daniels:2012dg}.
We chose protein chains from the left-out family as positive test examples. 
Negative test examples were protein chains from all other superfamilies in SCOP 
classes 1, 2, 3 and 4 (including other barrel superfamilies), indicated as 
representatives from the nr-PDB (\cite{Berman:2000hl}) database with 
non-redundancy set to a BLAST E-value of $10^{-7}$.

We used MRFy's local search mode (see Section~\ref{localsearch}) to align each
test example to the trained MRF.
The score reported for MRFy was the combined HMM and pairwise score from the 
MRF, which is identical to the SMURF energy function.
For each training set, the scores for both methods (MRFy with and without
simulated evolution) were collected and a ROC curve (a plot of true positive 
rate versus false positive rate) computed. We report the area under the curve 
(AUC statistic) from this ROC curve (\cite{Sonego:2008uy}).



%%%%%%
\section{Results}

\subsection{Search strategies}

For the three stochastic search approaches, we compared the raw score achieved
by each approach under a variety of conditions, as discussed in 
Section~\ref{searchstrats}.
The raw score is simply the negative log of the probability of the best path
found through the model.
Thus, raw scores are not comparable between models, but they are comparable
between query sequences for a given model.

Table~\ref{ss-propeller} indicates the performance of different stochastic
search techniques on the 8-bladed $\beta$-propeller fold.
While the simulated annealing and genetic algorithm approaches exhibit less
variance (a smaller standard deviation) from run to run, they do not approach
the minimum score of the local search approaches. Multi-start simulated 
annealing with a population of 10 and a convergence threshold of 200 generations
averages 29.3 seconds per search, but only achieves a minimum score of 2112, 
in contrast to a global optimum of 1781, a ratio of 0.843, though it converged 
in all cases.

In contrast, local search, given 30 seconds, achieves a minimum score of 1982 (a ratio of 0.899), 
and even in only 10 seconds achieves a minimum score of 1992 (a ratio of 0.894).
However, the global minimum score of 1781, which is achieved by SMURF on the
8-bladed $\beta$-propeller template, is only reached by MRFy with local search
two out of ten times, and this result required local search be allowed to run
for twenty minutes.
Thus, for this problem domain, local search seems to outperform our simulated
annealing and genetic algorithm implementations.


Table~\ref{ss-barwin} indicates the performance of the stochastic
search techniques on the ``Barwin-like endoglucanases'' $\beta$-barrel 
superfamily.
These structures are less complex than the propellers, even though they are
\emph{more} computationally complex for SMURFLite~\cite{Daniels:2012dg} if an 
interleave threshold greater than 2 is used. 
We see less variance than with the propellers, but once again, the local search
technique achieves a lower minimum score than simulated annealing or the genetic
algorithm.

Notably, local search achieves a minimum score of 978, which an 
\emph{exhaustive} search indicates to be a global minimum for this sequence on
this template.
With a time limit of 10 seconds, local search found this global minimum in one 
out of ten runs. 
With a time limit of 30 seconds, local search found it in two out of
ten runs, and with a time limit of 5 minutes, in four out of ten runs.

\rowcolors{2}{gray!25}{white}

\begin{small}
% \begin{center}
\begin{table*}[htb]
\centering
\caption{Stochastic search performance on 8-bladed $\beta$-propeller \label{ss-propeller}}
\begin{tabular}{lllll}
\hline
             & Min Score & Mean Score & Std Score & Mean Time (s) \\
\hline
SA 200       & 2112      & 2139       & 12.2      & 29.3      \\
\hline                                                        
SA 500       & 2129      & 2146       & 9.3       & 1020      \\
\hline                                                        
SA 1000      & 2112      & 2130       & 7.8      & 3314      \\
\hline                                                        
GA 1000/10   & 2105      & 2126       & 6.6      & 285      \\
\hline                                                        
GA 1000/50   & 2094      & 2118       & 7.7      & 1239      \\
\hline                                                        
GA 1000/100  & 2107      & 2120       & \textbf{3.8}      & 548      \\
\hline                                                        
GA 10000/10  & 2087      & 2111       & 7.2      & 5809      \\
\hline                                                        
GA 10000/50  & 2094      & 2112       & 7.1      & 5174      \\
\hline                                                        
GA 10000/100 & 2079      & 2114       & 9.0      & 10226      \\
\hline                                                        
LS 10s       & 1992     & 2015      & 19.4     & \textbf{10}    \\
\hline                                                        
LS 30s       & 1982     & 1991      & 10.9     & 30    \\
\hline                                                        
LS 5m        & \textbf{1818}     & \textbf{1876}      & 37.2     & 300    \\
\hline
\end{tabular}\\
{Performance of stochastic search techniques on an 8-bladed $\beta$-propeller
template. SA is Simulated Annealing, GA is Genetic Algorithm, and LS is Local
Search. For Simulated Annealing, we show results for convergence thresholds of
200, 500, and 1000 generations. 
For the Genetic Algorithm, we show results for convergence thresholds of 10, 
50, and 100 generations, and for population sizes of 1000 and 10000.
For Local Search, we show results for time limits of 10 seconds, 30 seconds and 
five minutes, on a 12-core AMD Opteron.
MRFy never achieved the global optimum score of 1781, achieved by SMURF, on this
template, except when local search was given 20 minutes of compute time, in
which case it found the global optimum two out of ten times.
These results show that local search outperforms the other search strategies,
and presents the tradeoff between the running time and accuracy of stochastic 
search.}
\end{table*}
% \end{center}
\end{small}

\begin{small}
% \begin{center}
\begin{table*}[htb]
  \centering
\caption{Stochastic search performance on ``Barwin-like'' $\beta$-barrel \label{ss-barwin}}
\begin{tabular}{llllll}
\hline
             & Min Score & Mean Score & Std Score & Mean Time (s) & Optimal \\
\hline
SA 200       & 1064      & 1071       & 3.8      & 79.5  & 0    \\
\hline                                                        
SA 500       & 1047      & 1063       & 7.6      & 104   & 0   \\
\hline                                                        
SA 1000      & 1024      & 1047       & 14.0      & 523  & 0    \\
\hline                                                        
GA 1000/10   & 1061      & 1069       & 3.6      & 232   & 0   \\
\hline                                                        
GA 1000/50   & 1059      & 1066       & 3.1      & 442    & 0  \\
\hline                                                        
GA 1000/100  & 1058      & 1069       & 4.0      & 1382   & 0   \\
\hline                                                        
GA 10000/10  & 1058      & 1063       & 2.5      & 8205   & 0   \\
\hline                                                        
GA 10000/50  & 1059      & 1061       & \textbf{2.2}      & 10306   & 0   \\
\hline                                                        
GA 10000/100 & 1057      & 1061       & \textbf{2.2}      & 16395   & 0   \\
\hline                                                        
LS 10s       & \textbf{978}     & 995      & 16.2     & \textbf{10} & 0.1   \\
\hline                                                        
LS 30s       & \textbf{978}     & 987      & 6.9     & 30  & 0.2  \\
\hline                                                        
LS 5m        & \textbf{978}     & \textbf{981}      & 2.9     & 300  & 0.4  \\
\hline
\end{tabular}\\
{Performance of stochastic search techniques on the ``Barwin-like 
endoglucanases'' $\beta$-barrel template. 
SA is Simulated Annealing, GA is Genetic Algorithm, and LS is Local Search. 
For Simulated Annealing, we show results for convergence thresholds of
200, 500, and 1000 generations. 
For the Genetic Algorithm, we show results for convergence thresholds of 10, 
50, and 100 generations, and for population sizes of 1000 and 10000.
For Local Search, we show results for time limits of 10 seconds, 30 seconds and 
five minutes, on a 12-core AMD Opteron.
The ``Optimal'' column indicates the fraction of runs for each search method
that achieved the global optimum.
These results show that local search outperforms the other search strategies,
and presents the tradeoff between the running time and accuracy of stochastic 
search.}
\end{table*}
% \end{center}
\end{small}

Table~\ref{ss-sandwich} indicates the performance of the stochastic
search techniques on the ``Concanavalin A-like lectins/glucanases'' 
$\beta$-sandwich superfamily.
These structures are also more complex than the propellers, even though they 
are also more computationally complex for SMURFLite with an interleave 
threshold greater than 2. 
On this superfamily, there is a closer overlap between the minimum score 
achieved by simulated annealing, at 790 (a ratio of 0.701 compared to the 
global minimum), and the range seen by local search;
local search with a time limit of 30 seconds achieves a \emph{mean} minimum
score of 791 (a ratio of 0.700), though its best was 740 (a ratio of 0.748).

Notably, when given a time limit of 5 minutes, local search achieved the 
\emph{global minimum} of 554 (as determined by exhaustive search) ten out of 
ten times. Local search never found this score when given only 10 seconds or
30 seconds as a time limit.

\begin{small}
% \begin{center}
\begin{table*}[!t]
  \centering
\caption{Stochastic search performance on $\beta$-sandwich\label{ss-sandwich}}
\begin{tabular}{llllll}
\hline
             & Min Score & Mean Score & Std Score & Mean Time (s) & Optimal \\
\hline
SA 200       & 795      & 834       & 18.6      & 84.7    & 0  \\
\hline                                                        
SA 500       & 790      & 820       & 17.3      & 192    & 0  \\
\hline                                                        
SA 1000      & 791      & 811       & 14.7      & 493    & 0  \\
\hline                                                        
GA 1000/10   & 874      & 888       & 4.1      & 1869    & 0  \\
\hline                                                        
GA 1000/50   & 878      & 883       & \textbf{2.5}      & 1305   & 0   \\
\hline                                                        
GA 1000/100  & 865      & 878       & 5.6      & 4309    & 0  \\
\hline                                                        
GA 10000/10  & 872      & 877       & \textbf{2.5}      & 6999   & 0   \\
\hline                                                        
GA 10000/50  & 875      & 879       & 3.1      & 5317    & 0  \\
\hline                                                        
GA 10000/100 & 869      & 875       & 4.5      & 10733   & 0   \\
\hline                                                        
LS 10s       & 771     & 826      & 31.7     & \textbf{10}  & 0  \\
\hline                                                  
LS 30s       & 740     & 791      & 47.0     & 30  & 0  \\
\hline                                                  
LS 5m        & \textbf{554}     & \textbf{554}      & \textbf{0.0}     & 300 & 1.0   \\
\hline
\end{tabular}\\
{Performance of stochastic search techniques on a ``Concanavalin A-like 
lectins/glucanases'', a 12-stranded $\beta$-sandwich template. 
SA is Simulated Annealing, GA is Genetic Algorithm, and LS is Local
Search. 
For Simulated Annealing, we show results for convergence thresholds of
200, 500, and 1000 generations. 
For the Genetic Algorithm, we show results for convergence thresholds of 10, 
50, and 100 generations, and for population sizes of 1000 and 10000.
For Local Search, we show results for time limits of 10 seconds, 30 seconds and 
five minutes, on a 12-core AMD Opteron.
The ``Optimal'' column indicates the fraction of runs for each search method
that achieved the global optimum.
These results show that local search outperforms the other search strategies,
and presents the tradeoff between the running time and accuracy of stochastic 
search.}
\end{table*}
% \end{center}
\end{small}

\rowcolors{2}{white}{white}



Our Haskell implementation made it exceedingly easy to parallelize MRFy across
multiple processing cores.
By default, MRFy will take advantage of all processing cores on a system; we
tested the parallel speedup on a system with 48 processing cores.
We measured the run-time performance of MRFy's genetic algorithm implementation
(with a fixed random seed) on the ``8-bladed $\beta$-propeller'' template.
The model has 343 nodes, of which 178 appear in 40 $\beta$-strands.
The segments between $\beta$-strands typically have at most 10 nodes.
We~used a query sequence of 592 amino acids, but each placement breaks
the sequence into 41~pieces, each of which typically has at most 20 amino
acids.
Because MRFy can solve the models between the $\beta$-strands independently,
this benchmark has a lot of parallelism.
%\enlargethispage{\baselineskip}

Figure~\ref{speedup} shows speedups when using from 
 1 to 48 of the cores 
on a 48-core, 2.3GHz AMD Opteron 6176 system.
Errors are estimated from 5 runs.
After about 12 cores, where MRFy runs 6 times as fast as sequential code, 
speedup rolls off.
We note that by running 4 instances of MRFy in parallel on different searches,
we would expect to be able to use all 48 cores with about 50\% efficiency.

Based on our results described above, we decided to have MRFy default to local 
search, with a time limit of 30 seconds.
We further explore MRFy's performance with and without simulated evolution,
and with initial guesses derived from SMURFLite.

\begin{figure}[htb!]
\begin{center}
  \fbox{\includegraphics[width=2.8in]{speedup.pdf}}
   \caption{MRFy's parallel speedup on an 8-bladed $\beta$-propeller, using a
   48-core system.
   After about 12 cores, speedup falls off.}
   \label{speedup}
 \end{center}
\end{figure}

\subsection{Remote homology detection accuracy}

We compared MRFy using local search and a 30-second time limit, against 
HMMER~\cite{Eddy:1998ut},
Raptor~\cite{Xu:2003p3417}, HHPred~\cite{Soding:2005ff}, 
and CNFPred~\cite{JianzhuMa:2012cp}.
We performed cross-validation testing on 11 $\beta$-barrel superfamilies, both
with and without simulated evolution.
For MRFy, the balance between accuracy and computational efficiency is 
determined by the termination conditions, as well as the search technique 
chosen.
Because local search so dramatically outperformed simulated annealing and the
genetic algorithm at obtaining better-scoring alignments, we conducted these 
cross-validation tests only on local search, using a simple time limit as a
termination condition.
We chose 30 seconds as a balance between speed and accuracy; a 5 minute time
limit might result in better accuracy, but for high-throughput, whole-genome 
scans, 5 minutes per alignment is excessive.

We compared MRFy's performance, both with and without simulated evolution, to 
the results from \citet{Daniels:2012dg}.
Table~\ref{mrfy-auc} shows the area (AUC) under the Receiver Operator 
Characteristic (ROC) curve for MRFy, the very best result from SMURFLite, and
HMMER, RAPTOR, and HHPred.
Importantly, we are choosing the best SMURFLite parameters for each superfamily,
which could not be known in advance; thus, we demonstrate improvements over the
\emph{very best} SMURFLite can perform, rather than just an average case.

We first note the ``Barwin-like endoglucanases'' superfamily highlighted in
\citet{Daniels:2012dg}.
SMURFLite performed better as the interleave threshold was increased on this
superfamily, and also when simulated evolution was added.
Since MRFy discards no $\beta$-strands, we were curious how it would perform
on this superfamily.
Notably, this superfamily has exceedingly little training data; during 
cross-validation, there are at most 4 training sequences and as few as 3
when filtered at a BLAST E-value of $10^{-7}$ and the family under test is left
out.
Without simulated evolution, MRFy achieves an AUC of 0.86, outperforming 
SMURFLite
without simulated evolution (SMURFLite achieved an AUC of 0.77 with an 
interleave threshold of 2, and 0.81 with an interleave threshold of 4).
When simulated evolution is added, MRFy achieves an AUC of 0.92, outperforming
SMURFLite with an interleave threshold of 2, but falling just short of the 0.94
AUC SMURFLite demonstrates with an interleave threshold of 4 and simulated 
evolution.
We note that the running time required for SMURFLite with an interleave
threshold of 4 was roughly 10 minutes; MRFy required only 30 seconds to achieve
comparable results.

Without using SMURFLite to provide an initial guess, MRFy outperforms SMURFLite 
in terms of AUC on four of the $\beta$-barrel 
superfamilies, while SMURFLite outperforms MRFy on three.
With SMURFLite initial guesses, MRFy outperformed SMURFLite on all but one
superfamily.
In particular, we note improvement on the ``Nucleic acid-binding proteins''
superfamily; see Figure~\ref{roc-curves} for the results of ROC analysis.
In general, MRFy improved the performance on the superfamilies on which
SMURFLite performed well, though it did not improve the performance on those
superfamilies for which 
SMURFLite had not performed well in our previous work.
Thus, with the exception of the ``Barwin-like endoglucanase'' superfamily, the
added $\beta$-strand information does not seem to help MRFy significantly in 
the cases where HMMER, RAPTOR, or HHPred performed best.


\begin{figure}[htb!]
\begin{center}
  \fbox{\includegraphics[width=2.8in]{50249-roc.pdf}}
   \caption{ROC curves and AUC statistic for the 
   ``Nucleic acid-binding proteins'' superfamily.
   }
   \label{roc-curves}
 \end{center}
\end{figure}


\rowcolors{2}{gray!25}{white}

\newcolumntype{H}[1] {%
>{\raggedright}%
p{#1}}

\begin{small}

\begin{center}
\begin{table*}[!t]
\caption{AUC on Beta-Barrel superfamilies\label{mrfy-auc}}
{\footnotesize \begin{tabular*}{\textwidth}{@{\extracolsep{\fill}}H{3.8cm}p{1.0cm}p{1.1cm}p{1.1cm}p{1.0cm}p{1.6cm}p{0.9cm}p{0.9cm}p{0.9cm}}\hline
 & HMMER & RAPTOR & HHPred & CNFPred & SMURF\-Lite & MRFy$^{1}$ & MRFy$^{2}$ & MRFy$^{3}$\\
 \hline
{\bf MRFy performs best}  & & & & & & & & \\
\hline
Translation proteins & - & - & 0.66 & 0.68 & 0.93 & {\bf 0.95} & 0.91 & {\bf 0.95}\\
Barwin-like endoglucanases & - & - & 0.75 & 0.79 & 0.77 & 0.86 & {\bf 0.92} & {\bf 0.94} \\
Tudor/PWWP/MBT & 0.78 & 0.74 & 0.67 & 0.83 & 0.83 & {\bf 0.86} & {\bf 0.86} & {\bf 0.86}\\
Nucleic acid-binding proteins & 0.75 & - & 0.67 & 0.66 & 0.92 & 0.75 & {\bf 0.95} & {\bf 0.95} \\
 % \hline
 % {\bf SMURFLite performs best}  & & & & & & & &  \\
% \hline
Cyclophilin-like & 0.67 & 0.61 & 0.7 & 0.68 & {\bf 0.85} & 0.82 & 0.80 & {\bf 0.85} \\ 
Sm-like ribonucleoproteins & 0.73 & 0.71 & 0.77 & 0.75 & 0.85 & 0.77 & 0.77 & {\bf 0.87} \\
Prokaryotic SH3-related domain & 0.81 & -  & - & - & 0.83 & 0.73 & 0.72 & {\bf 0.84} \\
 \hline
{\bf HHPred performs best}  & & & & & & & &\\
\hline
Translation proteins SH3-like & 0.83 & 0.81 & {\bf 0.86} & 0.71 & 0.62 & - & 0.63 & 0.63\\
 \hline
 {\bf CNFPred performs best} & & & & & & & &\\
\hline
PDZ domain-like & 0.96 & {\bf 1.0} & 0.99 & {\bf 1.0} & 0.97 & 0.95 & 0.95 & 0.96 \\
FMN-binding split barrel & 0.62 & 0.82 & 0.61 & {\bf 0.93} & - & - & - & -\\
 \hline
{\bf HMMER performs best}  & & & & & & & &  \\
\hline
Electron Transport accessory proteins & {\bf 0.84} & - & 0.77 & - & 0.66 & - & 0.68  & 0.68\\
\hline
\end{tabular*}}\\{Note: for SMURFLite, value indicated is the best of all
values from~\cite{Daniels:2012dg}. 
MRFy legend: [1]: MRFy without simulated evolution. 
[2]: MRFy with simulated evolution. 
[3]: MRFy with simulated evolution and initial guess provided by SMURFLite.
A dash ('-') in a result entry indicates the method failed on these structures, 
i.e. an AUC of less than 0.6}
\end{table*}
\end{center}

\end{small}

\rowcolors{0}{white}{white}



\section{Discussion}

We have presented MRFy, a method that uses stochastic search to find 
alignments of protein sequences to Markov random field models.
MRFy outperforms SMURFLite in most cases, but we should consider several 
possible enhancements to MRFy that might improve its performance.
As demonstrated on the $\beta$-sandwich superfamily, MRFy with local search 
achieves a globally optimal alignment when given 5 minutes of run-time, but 
fails to find a score close to this when given only 30 seconds.
It was not immediately clear how to bring convergence testing into the local
search model, but doing so might achieve results comparable to the 5 minute
results in less time.

We hope that MRFy will be useful for whole-genome annotation of newly-sequenced
organisms.
The tradeoff of time versus accuracy suggests a two-phase approach to this task:
a scan with relatively strict run-time performance requirements (perhaps no more
than ten seconds per alignment) coupled with a relatively loose $p$-value 
threshold would produce a number of candidates, many of which would likely be
false positives.
Then, MRFy could be re-run on these candidates with more computationally
demanding settings, and with a more strict $p$-value threshold.
MRFy computes $p$-values identically to SMURFLite: an extreme value 
distribution~\cite{Eddy:1998ut} is fitted to a distribution of raw scores,
and then a $p$-value is computed as $1-cdf\left( x \right)$ for any raw MRFy 
score $x$.
Computing the $p$-value accurately in the face of different search intensities
might require fitting multiple distributions, each for a different level of
search intensity.
Otherwise, if the distribution is obtained with an intensive search, then at
less-intensive search parameters, true positives may result in poor $p$-values;
similarly, if the distribution is obtained with a quick search, then 
more-intensive search parameters might result in false positives scoring 
comparatively well, and appearing to have good $p$-values.

As in~\cite{Daniels:2012dg}, we compared MRFy to 
HHPred~\cite{Soding:2005ff}.
As discussed, HHPred has an advantage in that it builds profiles based on all
of protein sequence space.
As a future enhancement to MRFy, we plan to introduce query profiles, so that
the MRFy alignment is to a sequence profile built from the query sequence,
rather than just the query sequence.
However, this will introduce a run-time performance hit in two ways.
First, the time to run a sequence homology search using the BLAST~\cite{Altschul:1997tl} 
family of tools can be significant, though the work on 
compressively-accelerated algorithms by Loh, et al.\cite{Loh:2012br} may reduce this 
impact.
Second, computing the Viterbi and $\beta$-pairing scores na\"{i}vely will
require time directly proportional to the number of sequences in the query
profile.
Representing these query sequences as sets of residue frequency vectors should
help, and there may be other approaches to consider.

Thus far, only $\beta$-strand interactions lead to non-local interactions in the
MRFy Markov random field.
In the future, we will investigate fitting other secondary structural elements
(the $\alpha$-helices) into this model.
In addition, disulfide bonds, which can occur between cysteine residues and
have been shown to be highly conserved~\cite{Naamati:2009eg, Tirosh:2012iq}, 
would appear to fit easily into this model.

\section{Acknowledgements}
Thanks to Richard Senington for the original local search implementation, Jian
Peng for useful discussions, and
Sheng Wang for providing access to CNFPred for comparison purposes.
This work was funded in part by NIH grant 1R01GM080330-01A1 (to L.C.).



{\small 
\bibliographystyle{abbrv}
\bibliography{mrfy}  }

\end{document}
